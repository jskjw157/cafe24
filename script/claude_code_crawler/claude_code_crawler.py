#!/usr/bin/env python3
"""
Claude Code Documentation Crawler
í¬ë¡¤ë§í•œ ë¬¸ì„œë¥¼ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
"""

import requests
from bs4 import BeautifulSoup
import html2text
import os
import time
import re
from urllib.parse import urljoin, urlparse, urlunparse
from bs4 import NavigableString
from pathlib import Path
import json

DOCS_OUTPUT_DIR = os.path.join("doc", "claude_code_docs")


class ClaudeCodeCrawler:
    def __init__(
        self,
        base_url="https://code.claude.com",
        output_dir=DOCS_OUTPUT_DIR,
        include_path_prefixes=None,
        exclude_path_patterns=None,
        compact_mode=True,
        keep_links=True,
    ):
        self.base_url = base_url
        self.output_dir = output_dir
        if include_path_prefixes is None:
            include_path_prefixes = ["/docs/en/"]
        self.include_path_prefixes = include_path_prefixes
        self.exclude_path_patterns = [
            re.compile(pattern) for pattern in (exclude_path_patterns or [])
        ]
        self.visited_urls = set()
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }
        self.html_converter = html2text.HTML2Text()
        self.compact_mode = compact_mode
        self.keep_links = keep_links
        self.html_converter.ignore_links = compact_mode and not keep_links
        self.html_converter.ignore_images = compact_mode
        self.html_converter.body_width = 0  # ì¤„ë°”ê¿ˆ ë°©ì§€
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        Path(output_dir).mkdir(parents=True, exist_ok=True)

    def strip_noise(self, content):
        """ë„¤ë¹„ê²Œì´ì…˜/í‘¸í„° ë“± ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°"""
        if content is None:
            return
        selectors = [
            'nav',
            'aside',
            'footer',
            'form',
            'button',
            '[role="navigation"]',
            '[role="search"]',
            '[aria-label="Search"]',
            '.sidebar',
            '.toc',
            '.table-of-contents',
            '.breadcrumbs',
            '.search',
            '.navigation',
            '.skip-to-content',
        ]
        for selector in selectors:
            for tag in content.select(selector):
                tag.decompose()
        for header in content.find_all('header'):
            if header.find('h1') is None:
                header.decompose()

    def trim_to_first_heading(self, content, heading_tag="h1"):
        """ì²« ë²ˆì§¸ ì œëª© ì´ì „ì˜ ì½˜í…ì¸  ì œê±°"""
        if content is None:
            return
        first_heading = content.find(heading_tag)
        if not first_heading:
            return
        node = first_heading
        while node and node is not content:
            prev = node.previous_sibling
            while prev:
                to_remove = prev
                prev = prev.previous_sibling
                if isinstance(to_remove, NavigableString):
                    to_remove.extract()
                else:
                    to_remove.decompose()
            node = node.parent

    def compress_markdown(self, markdown):
        """ë§ˆí¬ë‹¤ìš´ í¬ê¸° ì¶•ì†Œ"""
        if not markdown:
            return markdown
        markdown = markdown.replace("\u200b", "")
        skip_exact = {
            "Copy",
            "Copied",
            "Skip to main content",
            "Search",
            "Navigation",
        }
        lines = []
        in_code_block = False
        prev_blank = False
        skip_next_blank = False
        for raw_line in markdown.splitlines():
            line = raw_line.rstrip()
            stripped = line.strip()
            if stripped.startswith("```"):
                if not in_code_block:
                    while lines and lines[-1] == "":
                        lines.pop()
                else:
                    skip_next_blank = True
                in_code_block = not in_code_block
                lines.append(line)
                prev_blank = False
                continue
            if not in_code_block:
                if stripped in skip_exact:
                    continue
                if re.match(r"^#+\s*$", stripped):
                    continue
                if stripped in {"âŒ˜K"}:
                    continue
            if not stripped:
                if skip_next_blank:
                    continue
                if prev_blank:
                    continue
                prev_blank = True
                lines.append("")
                continue
            if skip_next_blank:
                skip_next_blank = False
            prev_blank = False
            lines.append(line)
        return "\n".join(lines).strip()
        
    def is_valid_doc_url(self, url):
        """ë¬¸ì„œ URLì¸ì§€ í™•ì¸"""
        parsed = urlparse(url)
        path = parsed.path or ""
        return (
            parsed.netloc == "code.claude.com" and
            "/docs/" in path and
            (
                not self.include_path_prefixes
                or any(path.startswith(prefix) for prefix in self.include_path_prefixes)
            ) and
            not path.endswith(('.png', '.jpg', '.jpeg', '.gif', '.css', '.js')) and
            not any(pattern.search(path) for pattern in self.exclude_path_patterns)
        )

    def normalize_url(self, url):
        """ì¿¼ë¦¬/í”„ë˜ê·¸ë¨¼íŠ¸ ì œê±°"""
        parsed = urlparse(url)
        return urlunparse((parsed.scheme, parsed.netloc, parsed.path, "", "", ""))
    
    def get_page_content(self, url):
        """í˜ì´ì§€ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°"""
        try:
            response = requests.get(url, headers=self.headers, timeout=15)
            response.raise_for_status()
            return response.text
        except Exception as e:
            print(f"âŒ Error fetching {url}: {str(e)}")
            return None
    
    def extract_main_content(self, soup):
        """ë©”ì¸ ì½˜í…ì¸  ì¶”ì¶œ"""
        # ë‹¤ì–‘í•œ ì„ íƒì ì‹œë„
        selectors = [
            'main',
            'article',
            '.docs-content',
            '.documentation',
            '[role="main"]',
            '#main-content'
        ]
        
        for selector in selectors:
            content = soup.select_one(selector)
            if content:
                return content
        
        # ì°¾ì§€ ëª»í•˜ë©´ body ë°˜í™˜
        return soup.find('body')
    
    def clean_filename(self, url):
        """URLì—ì„œ íŒŒì¼ëª… ìƒì„±"""
        parsed = urlparse(url)
        path = parsed.path.strip('/')
        
        # /docs/en/ ì œê±°
        path = re.sub(r'^docs/(en|ko)/', '', path)
        
        # ë¹ˆ ê²½ë¡œëŠ” indexë¡œ
        if not path or path == 'docs':
            path = 'index'
        
        # ì•ˆì „í•œ íŒŒì¼ëª…ìœ¼ë¡œ ë³€í™˜
        filename = path.replace('/', '_') + '.md'
        return filename
    
    def html_to_markdown(self, html_content):
        """HTMLì„ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ë³€í™˜"""
        try:
            markdown = self.html_converter.handle(str(html_content))
            # ë¶ˆí•„ìš”í•œ ê³µë°± ì •ë¦¬
            markdown = re.sub(r'\n{3,}', '\n\n', markdown)
            markdown = markdown.strip()
            if self.compact_mode:
                markdown = self.compress_markdown(markdown)
            return markdown
        except Exception as e:
            print(f"âš ï¸  Markdown conversion error: {str(e)}")
            return str(html_content)
    
    def extract_links(self, soup, current_url):
        """í˜ì´ì§€ì—ì„œ ë¬¸ì„œ ë§í¬ ì¶”ì¶œ"""
        links = set()
        for a_tag in soup.find_all('a', href=True):
            href = a_tag['href']
            full_url = urljoin(current_url, href)

            full_url = self.normalize_url(full_url)

            if self.is_valid_doc_url(full_url):
                links.add(full_url)
        
        return links
    
    def save_markdown(self, url, content, title=""):
        """ë§ˆí¬ë‹¤ìš´ íŒŒì¼ë¡œ ì €ì¥"""
        filename = self.clean_filename(url)
        filepath = os.path.join(self.output_dir, filename)
        
        # ë©”íƒ€ë°ì´í„° ì¶”ê°€
        metadata = f"""---
source: {url}
title: {title}
---

"""
        
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(metadata + content)
        
        print(f"âœ… Saved: {filename}")
        return filepath
    
    def crawl_page(self, url):
        """ë‹¨ì¼ í˜ì´ì§€ í¬ë¡¤ë§"""
        if url in self.visited_urls:
            return set()
        
        print(f"\nğŸ” Crawling: {url}")
        self.visited_urls.add(url)
        
        # í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°
        html_content = self.get_page_content(url)
        if not html_content:
            return set()
        
        # HTML íŒŒì‹±
        soup = BeautifulSoup(html_content, 'html.parser')

        # ë§í¬ ì¶”ì¶œ (ì •ì œ ì „ì— ìˆ˜í–‰í•´ì„œ ë§í¬ ì†ì‹¤ ë°©ì§€)
        new_links = self.extract_links(soup, url)

        # ì œëª© ì¶”ì¶œ
        title = soup.title.string if soup.title else ""

        # ë©”ì¸ ì½˜í…ì¸  ì¶”ì¶œ
        main_content = self.extract_main_content(soup)
        self.strip_noise(main_content)
        self.trim_to_first_heading(main_content)
        
        # ë§ˆí¬ë‹¤ìš´ ë³€í™˜
        markdown_content = self.html_to_markdown(main_content)
        
        # ì €ì¥
        self.save_markdown(url, markdown_content, title)

        return new_links
    
    def crawl(self, start_url, max_pages=50):
        """ì „ì²´ ë¬¸ì„œ í¬ë¡¤ë§"""
        print(f"ğŸš€ Starting crawl from: {start_url}")
        print(f"ğŸ“ Output directory: {self.output_dir}")
        print(f"ğŸ“„ Max pages: {max_pages}\n")
        
        to_visit = {start_url}
        pages_crawled = 0
        
        while to_visit and pages_crawled < max_pages:
            url = to_visit.pop()
            
            try:
                new_links = self.crawl_page(url)
                
                # ì•„ì§ ë°©ë¬¸í•˜ì§€ ì•Šì€ ë§í¬ë§Œ ì¶”ê°€
                for link in new_links:
                    if link not in self.visited_urls:
                        to_visit.add(link)
                
                pages_crawled += 1
                
                # ì„œë²„ ë¶€í•˜ ë°©ì§€
                time.sleep(1)
                
            except Exception as e:
                print(f"âŒ Error crawling {url}: {str(e)}")
                continue
        
        print(f"\nâœ¨ Crawling complete!")
        print(f"ğŸ“Š Pages crawled: {pages_crawled}")
        print(f"ğŸ“ Files saved in: {self.output_dir}")
        
        # í¬ë¡¤ë§ í†µê³„ ì €ì¥
        stats = {
            "total_pages": pages_crawled,
            "visited_urls": list(self.visited_urls),
            "output_dir": self.output_dir
        }
        
        with open(os.path.join(self.output_dir, '_crawl_stats.json'), 'w', encoding='utf-8') as f:
            json.dump(stats, f, indent=2, ensure_ascii=False)


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # í¬ë¡¤ëŸ¬ ìƒì„±
    crawler = ClaudeCodeCrawler(
        base_url="https://code.claude.com",
        output_dir=DOCS_OUTPUT_DIR
    )
    
    # ì‹œì‘ URL
    start_url = "https://code.claude.com/docs/en/overview"
    
    # í¬ë¡¤ë§ ì‹œì‘ (ìµœëŒ€ 50í˜ì´ì§€)
    crawler.crawl(start_url, max_pages=50)


if __name__ == "__main__":
    main()
